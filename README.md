# ğŸ™ï¸ Real-Time Fake Speech Detection System

With the increasing rise of AI-generated voices and deepfakes, ensuring the authenticity of voice-based interactions has become critical. This project aims to detect fake (AI-generated) vs. real (human) voices **in real time** during phone or audio/video calls.

---

## ğŸ” Problem Statement

AI-generated deepfake voices pose a serious threat to **phone-based communication**, security, and trust. This project addresses the challenge of distinguishing between **real human speech** and **synthetic audio** with high accuracy in real-time.

---

## ğŸ¯ Objectives

- âœ… Detect whether audio is **real** or **fake** during **live calls**.
- âœ… Segment audio streams and classify each segment in real time.
- âœ… Display live classification results during and after the call.
- âœ… Provide web dashboards and APIs for offline analysis.

---

## ğŸ“± Mobile App

Built using **Flutter** and integrated with **WebRTC** for **live audio and video calls**.

- Live call audio is continuously monitored.
- Each 1-second audio segment is analyzed and classified.
- Real-time prediction is shown during the call.
- Powered by REST APIs hosted on a custom **Apache server**.

---

## ğŸŒ Dashboards

Two interactive **web dashboards** were developed:

1. **Upload Audio Dashboard**: Upload `.wav` or `.mp3` files and classify audio segments.
2. **Analytics Dashboard**: Visualize segment-level predictions and compare models.

---

## ğŸ§  Machine & Deep Learning Models Used

### ğŸ”¹ Traditional Machine Learning Models
- **Random Forest** â€“ Baseline tree-based ensemble method.
- **SVM (Support Vector Machine)** â€“ Classifier using spectral features.
- **XGBoost** â€“ Gradient boosted decision trees.
- **Neural Network on FFT** â€“ Fully connected NN trained on FFT-transformed features.
- **MLP (Multilayer Perceptron)** â€“ Basic deep learning model using MFCC/log-mel inputs.

### ğŸ”¹ Deep Learning Architectures
- **VGG16** â€“ Fine-tuned on speech spectrogram images.
- **ResNet18** â€“ Residual CNN trained on MFCC features.
- **LSTM** â€“ Sequential model capturing temporal audio dependencies.
- **Transformer** â€“ Raw waveform ingestion without handcrafted features.

---

## ğŸ§ª Model Pipeline

- Input audio is segmented into **1-second chunks**.
- Preprocessing:
  - MFCC / Log-mel / FFT / Raw waveform (depending on model)
- Each chunk is classified:  
  **`1 â†’ Real`**  
  **`0 â†’ Fake`**

All models follow a **uniform segment-level inference pipeline** to ensure consistent evaluation and deployment.

---

## ğŸŒ Server & Backend

- **Apache HTTP Server** hosts the prediction API.
- **Python Flask** (or Django, optionally) handles requests and returns predictions.
- Models are loaded and executed on server-side with preloaded weights.

---

## ğŸ§° Tech Stack

| Area               | Tools & Frameworks                          |
|--------------------|---------------------------------------------|
| Mobile App         | Flutter, WebRTC                             |
| Web Dashboards     | HTML/CSS, JavaScript, Flask/Django          |
| Backend API        | Apache Server, Python, Flask                |
| Audio Processing   | Librosa, Numpy, Scipy, Pydub                |
| Deep Learning      | PyTorch, TensorFlow, Scikit-learn           |
| Visualization      | Matplotlib, Seaborn, Plotly                 |

---
