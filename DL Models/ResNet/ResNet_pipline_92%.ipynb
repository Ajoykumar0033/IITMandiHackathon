{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "559a96c4-e117-4449-aa1f-b0cef652fddb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resnet18\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Audio preprocessing parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/_meta_registrations.py:25\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;129m@register_meta\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroi_align\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_roi_align\u001b[39m(\u001b[38;5;28minput\u001b[39m, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):\n\u001b[1;32m     27\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(rois\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrois must have shape as Tensor[K, 5]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         ),\n\u001b[1;32m     34\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torchvision\u001b[38;5;241m.\u001b[39mextension\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import argparse\n",
    "\n",
    "# Audio preprocessing parameters\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 3.0\n",
    "N_MELS = 128\n",
    "HOP_LENGTH = 512\n",
    "N_FFT = 2048\n",
    "\n",
    "# Define the model architecture (must match training)\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "def preprocess_audio(file_path):\n",
    "    # Load audio\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        audio = np.zeros(int(DURATION * SAMPLE_RATE))\n",
    "        sr = SAMPLE_RATE\n",
    "    \n",
    "    # Pad or trim to fixed length\n",
    "    target_length = int(DURATION * SAMPLE_RATE)\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    # Convert to mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=SAMPLE_RATE, n_mels=N_MELS, hop_length=HOP_LENGTH, n_fft=N_FFT\n",
    "    )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    # Normalize\n",
    "    mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    mel_spec_tensor = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # [1, 1, n_mels, time]\n",
    "    \n",
    "    return mel_spec_tensor\n",
    "\n",
    "def infer_audio(file_path, model_path=\"best_model.pt\"):\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = AudioClassifier()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess audio\n",
    "    input_tensor = preprocess_audio(file_path)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    # Return result\n",
    "    return \"FAKE\" if pred == 1 else \"REAL\"\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Perform inference on an audio file using a trained model.\")\n",
    "    parser.add_argument(\"/home/ub/Downloads/fake.wav\", type=str, help=\"Path to the audio file (.wav)\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"best_model.pt\", help=\"Path to the trained model file\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    result = infer_audio(args.audio_file, args.model_path)\n",
    "    print(f\"Prediction for {args.audio_file}: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8cec5ff-8fa8-45c0-a616-e869fb7c36c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resnet18\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/_meta_registrations.py:25\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;129m@register_meta\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroi_align\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_roi_align\u001b[39m(\u001b[38;5;28minput\u001b[39m, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):\n\u001b[1;32m     27\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(rois\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrois must have shape as Tensor[K, 5]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         ),\n\u001b[1;32m     34\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torchvision\u001b[38;5;241m.\u001b[39mextension\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Audio preprocessing parameters\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 3.0\n",
    "N_MELS = 128\n",
    "HOP_LENGTH = 512\n",
    "N_FFT = 2048\n",
    "\n",
    "# Define the model architecture (must match training)\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "def preprocess_audio(file_path=\"/home/ub/Downloads/fake.wav\"):\n",
    "    # Validate file existence\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n",
    "    \n",
    "    # Load audio\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        audio = np.zeros(int(DURATION * SAMPLE_RATE))\n",
    "        sr = SAMPLE_RATE\n",
    "    \n",
    "    # Pad or trim to fixed length\n",
    "    target_length = int(DURATION * SAMPLE_RATE)\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    # Convert to mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=SAMPLE_RATE, n_mels=N_MELS, hop_length=HOP_LENGTH, n_fft=N_FFT\n",
    "    )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    # Normalize\n",
    "    mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    mel_spec_tensor = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # [1, 1, n_mels, time]\n",
    "    \n",
    "    return mel_spec_tensor\n",
    "\n",
    "def infer_audio(file_path, model_path=\"best_model.pt\"):\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        model = AudioClassifier()\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading model from {model_path}: {e}\")\n",
    "    \n",
    "    # Preprocess audio\n",
    "    input_tensor = preprocess_audio(file_path)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    # Return result\n",
    "    return \"FAKE\" if pred == 1 else \"REAL\"\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Perform inference on an audio file using a trained model.\")\n",
    "    parser.add_argument(\"audio_file\", type=str, help=\"Path to the audio file (.wav)\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"best_model.pt\", help=\"Path to the trained model file\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    try:\n",
    "        result = infer_audio(args.audio_file, args.model_path)\n",
    "        print(f\"Prediction for {args.audio_file}: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cef6073-90ea-4400-9833-2f211e69b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ub/anaconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu121\n",
      "Torchvision version: 0.18.0+cu121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ub/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "usage: ipykernel_launcher.py [-h] [--model_path MODEL_PATH] audio_file\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ub/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check PyTorch and Torchvision versions\n",
    "try:\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    import torchvision\n",
    "    print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing torch or torchvision: {e}\")\n",
    "    print(\"Please ensure compatible versions are installed, e.g., torch==2.3.0 and torchvision==0.18.0\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Audio preprocessing parameters\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 3.0\n",
    "N_MELS = 128\n",
    "HOP_LENGTH = 512\n",
    "N_FFT = 2048\n",
    "\n",
    "# Define the model architecture (must match training)\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "def preprocess_audio(file_path):\n",
    "    # Validate file existence\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n",
    "    \n",
    "    # Load audio\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        audio = np.zeros(int(DURATION * SAMPLE_RATE))\n",
    "        sr = SAMPLE_RATE\n",
    "    \n",
    "    # Pad or trim to fixed length\n",
    "    target_length = int(DURATION * SAMPLE_RATE)\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    # Convert to mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=SAMPLE_RATE, n_mels=N_MELS, hop_length=HOP_LENGTH, n_fft=N_FFT\n",
    "    )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    # Normalize\n",
    "    mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    mel_spec_tensor = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # [1, 1, n_mels, time]\n",
    "    \n",
    "    return mel_spec_tensor\n",
    "\n",
    "def infer_audio(file_path, model_path=\"best_model.pt\"):\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        model = AudioClassifier()\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading model from {model_path}: {e}\")\n",
    "    \n",
    "    # Preprocess audio\n",
    "    input_tensor = preprocess_audio(file_path)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    # Return result\n",
    "    return \"FAKE\" if pred == 1 else \"REAL\"\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Perform inference on an audio file using a trained model.\")\n",
    "    parser.add_argument(\"audio_file\", type=str, help=\"Path to the audio file (.wav)\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"best_model.pt\", help=\"Path to the trained model file\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    try:\n",
    "        result = infer_audio(args.audio_file, args.model_path)\n",
    "        print(f\"Prediction for {args.audio_file}: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c48fbfa1-95f0-4997-9f3e-72038d1a1dda",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3398162282.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip uninstall torch torchvision torchaudio\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip uninstall torch torchvision torchaudio\n",
    "pip install torch==2.3.0 torchvision==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c9451-9891-4277-93d8-d0faa8422b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check PyTorch and Torchvision versions\n",
    "try:\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    import torchvision\n",
    "    print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing torch or torchvision: {e}\")\n",
    "    print(\"Please ensure compatible versions are installed.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Audio preprocessing parameters\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 3.0\n",
    "N_MELS = 128\n",
    "HOP_LENGTH = 512\n",
    "N_FFT = 2048\n",
    "\n",
    "# Define the model architecture (same as training)\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "def preprocess_audio(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        audio = np.zeros(int(DURATION * SAMPLE_RATE))\n",
    "    \n",
    "    target_length = int(DURATION * SAMPLE_RATE)\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=SAMPLE_RATE, n_mels=N_MELS, hop_length=HOP_LENGTH, n_fft=N_FFT\n",
    "    )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n",
    "    mel_spec_tensor = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    return mel_spec_tensor\n",
    "\n",
    "def infer_audio(file_path, model_path=\"best_model.pt\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = AudioClassifier()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    input_tensor = preprocess_audio(file_path).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred = torch.argmax(output, dim=1).cpu().item()\n",
    "    \n",
    "    return \"FAKE\" if pred == 1 else \"REAL\"\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Audio inference using a trained model.\")\n",
    "    parser.add_argument(\"audio_file\", type=str, help=\"Path to the input audio file (.wav)\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"best_model.pt\", help=\"Path to the model checkpoint\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    try:\n",
    "        result = infer_audio(args.audio_file, args.model_path)\n",
    "        print(f\"Prediction for {args.audio_file}: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# Main entry\n",
    "if __name__ == \"__main__\":\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        # Inside Jupyter/IPython\n",
    "        test_audio = \"path/to/your_audio.wav\"  # <-- Replace with real .wav file\n",
    "        model_path = \"best_model.pt\"\n",
    "        result = infer_audio(test_audio, model_path)\n",
    "        print(f\"Prediction for {test_audio}: {result}\")\n",
    "    else:\n",
    "        # CLI mode\n",
    "        main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
