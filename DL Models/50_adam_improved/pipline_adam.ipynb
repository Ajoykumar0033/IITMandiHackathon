{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49e7914c-046e-4984-975f-828837b6b4e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.library' has no attribute 'register_fake'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnoisereduce\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnr\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mobilenet_v2\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Constants (must match training)\u001b[39;00m\n\u001b[32m     10\u001b[39m SAMPLE_RATE = \u001b[32m16000\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flybrain_env/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flybrain_env/lib/python3.12/site-packages/torchvision/_meta_registrations.py:163\u001b[39m\n\u001b[32m    153\u001b[39m     torch._check(\n\u001b[32m    154\u001b[39m         grad.dtype == rois.dtype,\n\u001b[32m    155\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[32m   (...)\u001b[39m\u001b[32m    158\u001b[39m         ),\n\u001b[32m    159\u001b[39m     )\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grad.new_empty((batch_size, channels, height, width))\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;129m@torch\u001b[39m\u001b[43m.\u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_fake\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mtorchvision::nms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmeta_nms\u001b[39m(dets, scores, iou_threshold):\n\u001b[32m    165\u001b[39m     torch._check(dets.dim() == \u001b[32m2\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mboxes should be a 2d tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    166\u001b[39m     torch._check(dets.size(\u001b[32m1\u001b[39m) == \u001b[32m4\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mboxes should have 4 elements in dimension 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets.size(\u001b[32m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch.library' has no attribute 'register_fake'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import noisereduce as nr\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "# Constants (must match training)\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 3.0\n",
    "N_MELS = 64\n",
    "HOP_LENGTH = 512\n",
    "N_FFT = 1024\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model Architecture (identical to training)\n",
    "# Model Architecture (identical to training)\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.mobilenet = mobilenet_v2(pretrained=False)  # No need for pretrained weights\n",
    "        # Modify first layer for 1-channel input\n",
    "        self.mobilenet.features[0][0] = nn.Conv2d(\n",
    "            1, 32, kernel_size=3, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.mobilenet.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.mobilenet.classifier[1].in_features, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mobilenet(x)\n",
    "# Audio Preprocessing (identical to training)\n",
    "def preprocess_audio(file_path):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "        audio = nr.reduce_noise(y=audio, sr=SAMPLE_RATE, stationary=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        audio = np.zeros(int(DURATION * SAMPLE_RATE))\n",
    "    \n",
    "    # Pad/trim audio\n",
    "    target_length = int(DURATION * SAMPLE_RATE)\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    # Convert to mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=SAMPLE_RATE, n_mels=N_MELS, hop_length=HOP_LENGTH, n_fft=N_FFT\n",
    "    )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n",
    "    return torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Inference Function\n",
    "def classify_audio(file_path, model_path=\"best_model_50.pt\"):\n",
    "    # Load model\n",
    "    model = AudioClassifier()\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load model from {model_path}: {e}\")\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess and predict\n",
    "    with torch.no_grad():\n",
    "        input_tensor = preprocess_audio(file_path).to(DEVICE)\n",
    "        output = model(input_tensor)\n",
    "        pred = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    return \"FAKE\" if pred == 1 else \"REAL\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For Jupyter: Hardcode paths here\n",
    "    test_audio = \"/home/ub/Downloads/fake.wav\"  # Replace with your file\n",
    "    model_path = \"/home/ub/codes/DLheck_server/hcl_heck/50_adam/50_adam_improved/results_20250503_145754/best_model_50.pt\"  # Replace if needed\n",
    "    \n",
    "    result = infer_audio(test_audio, model_path)\n",
    "    print(f\"Prediction: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f1a417b-fe71-4df2-8401-b50bf733afb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchivision (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torchivision\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchivision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faedf85b-c018-46e5-a785-0b29299bdf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
