{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2abae86-6fbd-426e-83c4-04776dc17c09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from multiprocessing import Pool\n",
    "import os, glob, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaff531c-7c2f-4aa6-90f3-27381ad8a5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Select device and print it ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"[INFO] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b894e88e-1dfa-4850-9939-9156e6a2f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure you have Polars installed:\n",
    "# !pip install polars\n",
    "\n",
    "import os, glob, time, threading\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def _load_file_polars(fp_lbl):\n",
    "    \"\"\"\n",
    "    Read one CSV with Polars (uses its own thread pool),\n",
    "    extract 'magnitude' as a numpy float32 array.\n",
    "    \"\"\"\n",
    "    fp, lbl = fp_lbl\n",
    "    df = pl.read_csv(fp, columns=[\"magnitude\"])\n",
    "    df = df.with_columns(pl.col(\"magnitude\").cast(pl.UInt32))\n",
    "    return df[\"magnitude\"].to_numpy(), lbl\n",
    "\n",
    "def load_magnitude_data(base_dir, num_workers=None):\n",
    "    \"\"\"\n",
    "    Notebook-compatible, fully parallel loader with 2-second status prints.\n",
    "    \n",
    "    Args:\n",
    "      base_dir (str): root dir containing 'fake/' and 'real/' subfolders.\n",
    "      num_workers (int, optional): max threads; defaults to all CPU cores.\n",
    "    \n",
    "    Returns:\n",
    "      X (np.ndarray): stacked [n_samples, …] float32\n",
    "      y (np.ndarray): [n_samples] int64 labels (0=fake,1=real)\n",
    "    \"\"\"\n",
    "    cpu_cores = os.cpu_count() or 1\n",
    "    workers = num_workers or cpu_cores\n",
    "\n",
    "    # 1) discover files\n",
    "    file_list = [\n",
    "        (fp, lbl)\n",
    "        for lbl, cls in enumerate([\"fake\", \"real\"])\n",
    "        for fp in glob.glob(os.path.join(base_dir, cls, \"*.csv\"))\n",
    "    ]\n",
    "    total = len(file_list)\n",
    "    print(f\"[SETUP] Found {total} CSV files; using {workers} threads\")\n",
    "\n",
    "    # 2) progress printer\n",
    "    count = 0\n",
    "    stop_evt = threading.Event()\n",
    "    def _printer():\n",
    "        while not stop_evt.is_set():\n",
    "            time.sleep(2)\n",
    "            print(f\"[PROGRESS] Loaded {count}/{total} files\")\n",
    "    threading.Thread(target=_printer, daemon=True).start()\n",
    "\n",
    "    # 3) parallel load\n",
    "    X_parts, y_parts = [], []\n",
    "    t0 = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=workers) as exe:\n",
    "        futures = {exe.submit(_load_file_polars, item): item for item in file_list}\n",
    "        for future in as_completed(futures):\n",
    "            arr, lbl = future.result()\n",
    "            X_parts.append(arr)\n",
    "            y_parts.append(lbl)\n",
    "            count += 1\n",
    "\n",
    "    # 4) wrap up\n",
    "    stop_evt.set()\n",
    "    elapsed = time.time() - t0\n",
    "    X = np.vstack(X_parts)\n",
    "    y = np.array(y_parts, dtype=np.int64)\n",
    "    print(f\"[DONE] {elapsed:.1f}s → X.shape={X.shape}, y.shape={y.shape}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf82e37-69ae-4659-88e2-8043ed4a0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2edf2fbd-fdc2-43e8-9bb4-06bfd6671b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(X, y, batch_size=32, shuffle=False):\n",
    "    dataset = TensorDataset(torch.from_numpy(X), torch.from_numpy(y).unsqueeze(1).float())\n",
    "    loader  = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    print(f\"[DATALOADER] Created loader with {len(loader)} batches of size {batch_size}\")\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a27546-0424-443d-9d71-2bf702bdf65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOADING DATA ===\n",
      "[SETUP] Found 53868 CSV files; using 50 threads\n",
      "[PROGRESS] Loaded 1861/53868 files\n",
      "[PROGRESS] Loaded 5989/53868 files\n",
      "[PROGRESS] Loaded 10091/53868 files\n",
      "[PROGRESS] Loaded 14193/53868 files\n",
      "[PROGRESS] Loaded 18303/53868 files\n",
      "[PROGRESS] Loaded 22338/53868 files\n",
      "[PROGRESS] Loaded 26390/53868 files\n",
      "[PROGRESS] Loaded 30436/53868 files\n",
      "[PROGRESS] Loaded 34508/53868 files\n",
      "[PROGRESS] Loaded 38475/53868 files\n",
      "[PROGRESS] Loaded 42502/53868 files\n",
      "[PROGRESS] Loaded 44736/53868 files\n",
      "[PROGRESS] Loaded 45110/53868 files\n",
      "[PROGRESS] Loaded 45503/53868 files\n",
      "[PROGRESS] Loaded 45984/53868 files\n",
      "[PROGRESS] Loaded 46322/53868 files\n",
      "[PROGRESS] Loaded 46695/53868 files\n",
      "[PROGRESS] Loaded 47033/53868 files\n",
      "[PROGRESS] Loaded 47381/53868 files\n",
      "[PROGRESS] Loaded 47711/53868 files\n",
      "[PROGRESS] Loaded 48056/53868 files\n",
      "[PROGRESS] Loaded 48476/53868 files\n",
      "[PROGRESS] Loaded 48818/53868 files\n",
      "[PROGRESS] Loaded 49259/53868 files\n",
      "[PROGRESS] Loaded 49752/53868 files\n",
      "[PROGRESS] Loaded 50260/53868 files\n",
      "[PROGRESS] Loaded 50685/53868 files\n",
      "[PROGRESS] Loaded 51111/53868 files\n",
      "[PROGRESS] Loaded 51436/53868 files\n",
      "[PROGRESS] Loaded 51778/53868 files\n",
      "[PROGRESS] Loaded 52169/53868 files\n",
      "[PROGRESS] Loaded 52572/53868 files\n",
      "[PROGRESS] Loaded 53071/53868 files\n",
      "[PROGRESS] Loaded 53595/53868 files\n",
      "[PROGRESS] Loaded 53868/53868 files\n",
      "[DONE] 78.6s → X.shape=(53868, 39961), y.shape=(53868,)\n",
      "[SETUP] Found 10798 CSV files; using 50 threads\n",
      "[PROGRESS] Loaded 161/10798 files\n",
      "[PROGRESS] Loaded 484/10798 files\n",
      "[PROGRESS] Loaded 838/10798 files\n",
      "[PROGRESS] Loaded 1190/10798 files\n",
      "[PROGRESS] Loaded 1566/10798 files\n",
      "[PROGRESS] Loaded 2109/10798 files\n",
      "[PROGRESS] Loaded 2599/10798 files\n",
      "[PROGRESS] Loaded 2953/10798 files\n",
      "[PROGRESS] Loaded 3389/10798 files\n",
      "[PROGRESS] Loaded 3724/10798 files\n",
      "[PROGRESS] Loaded 4111/10798 files\n",
      "[PROGRESS] Loaded 4432/10798 files\n",
      "[PROGRESS] Loaded 4793/10798 files\n",
      "[PROGRESS] Loaded 5191/10798 files\n",
      "[PROGRESS] Loaded 5649/10798 files\n",
      "[PROGRESS] Loaded 6043/10798 files\n",
      "[PROGRESS] Loaded 6418/10798 files\n",
      "[PROGRESS] Loaded 6759/10798 files\n",
      "[PROGRESS] Loaded 7149/10798 files\n",
      "[PROGRESS] Loaded 7487/10798 files\n",
      "[PROGRESS] Loaded 7915/10798 files\n",
      "[PROGRESS] Loaded 8315/10798 files\n",
      "[PROGRESS] Loaded 8646/10798 files\n",
      "[PROGRESS] Loaded 9014/10798 files\n",
      "[PROGRESS] Loaded 9361/10798 files\n",
      "[PROGRESS] Loaded 9689/10798 files\n",
      "[PROGRESS] Loaded 10013/10798 files\n",
      "[PROGRESS] Loaded 10394/10798 files\n",
      "[PROGRESS] Loaded 10765/10798 files\n",
      "[DONE] 66.6s → X.shape=(10798, 39961), y.shape=(10798,)\n",
      "[SETUP] Found 4634 CSV files; using 50 threads\n",
      "[PROGRESS] Loaded 10798/10798 files\n",
      "[PROGRESS] Loaded 286/4634 files\n",
      "[PROGRESS] Loaded 620/4634 files\n",
      "[PROGRESS] Loaded 939/4634 files\n",
      "[PROGRESS] Loaded 1294/4634 files\n",
      "[PROGRESS] Loaded 1609/4634 files\n",
      "[PROGRESS] Loaded 1925/4634 files\n",
      "[PROGRESS] Loaded 2241/4634 files\n",
      "[PROGRESS] Loaded 2603/4634 files\n",
      "[PROGRESS] Loaded 2877/4634 files\n",
      "[PROGRESS] Loaded 3195/4634 files\n",
      "[PROGRESS] Loaded 3513/4634 files\n",
      "[PROGRESS] Loaded 3830/4634 files\n",
      "[PROGRESS] Loaded 4144/4634 files\n",
      "[PROGRESS] Loaded 4417/4634 files\n",
      "[DONE] 29.4s → X.shape=(4634, 39961), y.shape=(4634,)\n"
     ]
    }
   ],
   "source": [
    " print(\"=== LOADING DATA ===\")\n",
    "X_train, y_train = load_magnitude_data('./for-norm/for-norm/training', num_workers=50)\n",
    "X_val,   y_val   = load_magnitude_data('./for-norm/for-norm/validation', num_workers=50)\n",
    "X_test,  y_test  = load_magnitude_data('./for-norm/for-norm/testing', num_workers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cd0aba0-4ca9-4a43-b6f0-d1eee9296453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCALING FEATURES ===\n",
      "[SCALE] Feature means (first 5): [ 0.  0. -0.  0. -0.]\n",
      "[SCALE] Feature stddevs (first 5): [1. 1. 1. 1. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== SCALING FEATURES ===\")\n",
    "scaler   = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_val   = scaler.transform(X_val).astype(np.float32)\n",
    "X_test  = scaler.transform(X_test).astype(np.float32)\n",
    "print(f\"[SCALE] Feature means (first 5): {np.round(scaler.mean_[:5], 5)}\")\n",
    "print(f\"[SCALE] Feature stddevs (first 5): {np.round(np.sqrt(scaler.var_[:5]), 5)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bae6d77-358d-47f8-a5d5-b99d9db8b284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARING DATALOADERS ===\n",
      "[DATALOADER] Created loader with 1684 batches of size 32\n",
      "[DATALOADER] Created loader with 338 batches of size 32\n",
      "[DATALOADER] Created loader with 145 batches of size 32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== PREPARING DATALOADERS ===\")\n",
    "train_loader = make_loader(X_train, y_train, batch_size=32, shuffle=True)\n",
    "val_loader   = make_loader(X_val,   y_val,   batch_size=32, shuffle=False)\n",
    "test_loader  = make_loader(X_test,  y_test,  batch_size=32, shuffle=False)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c0c0b4f-4b0f-4c67-91bd-6d4f6a496feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUILDING MODEL ===\n",
      "VoiceClassifier(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=39961, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.5, inplace=False)\n",
      "    (9): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== BUILDING MODEL ===\")\n",
    "model     = VoiceClassifier(X_train.shape[1]).to(device)\n",
    "print(model)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "016f13e7-080a-41d8-aa4c-620288d198a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True   # enable cuDNN auto-tuner for best performance\n",
    "print(f\"[INFO] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9464a-0573-4f7a-bfce-9c16e30559bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e349c583-efa9-45d0-b463-ce4f23c56765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/20 ===\n",
      "[TRAIN] Batch 10/1684 - Avg Loss: 21.1243, Avg Acc: 0.5469\n",
      "[TRAIN] Batch 20/1684 - Avg Loss: 26.1742, Avg Acc: 0.5609\n",
      "[TRAIN] Batch 30/1684 - Avg Loss: 30.8561, Avg Acc: 0.5531\n",
      "[TRAIN] Batch 40/1684 - Avg Loss: 33.8437, Avg Acc: 0.5477\n",
      "[TRAIN] Batch 50/1684 - Avg Loss: 35.7949, Avg Acc: 0.5406\n",
      "[TRAIN] Batch 60/1684 - Avg Loss: 35.3140, Avg Acc: 0.5469\n",
      "[TRAIN] Batch 70/1684 - Avg Loss: 34.8630, Avg Acc: 0.5500\n",
      "[TRAIN] Batch 80/1684 - Avg Loss: 34.6031, Avg Acc: 0.5488\n",
      "[TRAIN] Batch 90/1684 - Avg Loss: 35.8709, Avg Acc: 0.5417\n",
      "[TRAIN] Batch 100/1684 - Avg Loss: 36.2780, Avg Acc: 0.5419\n",
      "[TRAIN] Batch 110/1684 - Avg Loss: 36.2081, Avg Acc: 0.5437\n",
      "[TRAIN] Batch 120/1684 - Avg Loss: 36.1214, Avg Acc: 0.5437\n",
      "[TRAIN] Batch 130/1684 - Avg Loss: 35.1385, Avg Acc: 0.5474\n",
      "[TRAIN] Batch 140/1684 - Avg Loss: 33.6574, Avg Acc: 0.5516\n",
      "[TRAIN] Batch 150/1684 - Avg Loss: 32.1388, Avg Acc: 0.5600\n",
      "[TRAIN] Batch 160/1684 - Avg Loss: 31.0232, Avg Acc: 0.5621\n",
      "[TRAIN] Batch 170/1684 - Avg Loss: 30.0551, Avg Acc: 0.5638\n",
      "[TRAIN] Batch 180/1684 - Avg Loss: 29.1987, Avg Acc: 0.5639\n",
      "[TRAIN] Batch 190/1684 - Avg Loss: 28.2369, Avg Acc: 0.5641\n",
      "[TRAIN] Batch 200/1684 - Avg Loss: 27.1539, Avg Acc: 0.5647\n",
      "[TRAIN] Batch 210/1684 - Avg Loss: 25.9102, Avg Acc: 0.5616\n",
      "[TRAIN] Batch 220/1684 - Avg Loss: 24.7648, Avg Acc: 0.5599\n",
      "[TRAIN] Batch 230/1684 - Avg Loss: 23.7192, Avg Acc: 0.5596\n",
      "[TRAIN] Batch 240/1684 - Avg Loss: 22.7612, Avg Acc: 0.5579\n",
      "[TRAIN] Batch 250/1684 - Avg Loss: 21.8786, Avg Acc: 0.5564\n",
      "[TRAIN] Batch 260/1684 - Avg Loss: 21.0760, Avg Acc: 0.5546\n",
      "[TRAIN] Batch 270/1684 - Avg Loss: 20.3314, Avg Acc: 0.5545\n",
      "[TRAIN] Batch 280/1684 - Avg Loss: 19.6420, Avg Acc: 0.5522\n",
      "[TRAIN] Batch 290/1684 - Avg Loss: 18.9887, Avg Acc: 0.5515\n",
      "[TRAIN] Batch 300/1684 - Avg Loss: 18.3799, Avg Acc: 0.5506\n",
      "[TRAIN] Batch 310/1684 - Avg Loss: 17.8604, Avg Acc: 0.5496\n",
      "[TRAIN] Batch 320/1684 - Avg Loss: 17.3900, Avg Acc: 0.5510\n",
      "[TRAIN] Batch 330/1684 - Avg Loss: 17.0224, Avg Acc: 0.5514\n",
      "[TRAIN] Batch 340/1684 - Avg Loss: 16.6717, Avg Acc: 0.5511\n",
      "[TRAIN] Batch 350/1684 - Avg Loss: 16.2422, Avg Acc: 0.5506\n",
      "[TRAIN] Batch 360/1684 - Avg Loss: 15.8186, Avg Acc: 0.5508\n",
      "[TRAIN] Batch 370/1684 - Avg Loss: 15.4279, Avg Acc: 0.5488\n",
      "[TRAIN] Batch 380/1684 - Avg Loss: 15.0403, Avg Acc: 0.5483\n",
      "[TRAIN] Batch 390/1684 - Avg Loss: 14.6731, Avg Acc: 0.5470\n",
      "[TRAIN] Batch 400/1684 - Avg Loss: 14.3234, Avg Acc: 0.5463\n",
      "[TRAIN] Batch 410/1684 - Avg Loss: 13.9990, Avg Acc: 0.5447\n",
      "[TRAIN] Batch 420/1684 - Avg Loss: 13.6822, Avg Acc: 0.5432\n",
      "[TRAIN] Batch 430/1684 - Avg Loss: 13.3802, Avg Acc: 0.5429\n",
      "[TRAIN] Batch 440/1684 - Avg Loss: 13.0917, Avg Acc: 0.5432\n",
      "[TRAIN] Batch 450/1684 - Avg Loss: 12.8161, Avg Acc: 0.5426\n",
      "[TRAIN] Batch 460/1684 - Avg Loss: 12.5528, Avg Acc: 0.5427\n",
      "[TRAIN] Batch 470/1684 - Avg Loss: 12.3003, Avg Acc: 0.5421\n",
      "[TRAIN] Batch 480/1684 - Avg Loss: 12.0585, Avg Acc: 0.5423\n",
      "[TRAIN] Batch 490/1684 - Avg Loss: 11.8452, Avg Acc: 0.5429\n",
      "[TRAIN] Batch 500/1684 - Avg Loss: 11.6401, Avg Acc: 0.5424\n",
      "[TRAIN] Batch 510/1684 - Avg Loss: 11.4331, Avg Acc: 0.5422\n",
      "[TRAIN] Batch 520/1684 - Avg Loss: 11.2503, Avg Acc: 0.5416\n",
      "[TRAIN] Batch 530/1684 - Avg Loss: 11.0572, Avg Acc: 0.5407\n",
      "[TRAIN] Batch 540/1684 - Avg Loss: 10.8652, Avg Acc: 0.5399\n",
      "[TRAIN] Batch 550/1684 - Avg Loss: 10.6806, Avg Acc: 0.5387\n",
      "[TRAIN] Batch 560/1684 - Avg Loss: 10.5023, Avg Acc: 0.5383\n",
      "[TRAIN] Batch 570/1684 - Avg Loss: 10.3302, Avg Acc: 0.5384\n",
      "[TRAIN] Batch 580/1684 - Avg Loss: 10.1639, Avg Acc: 0.5383\n",
      "[TRAIN] Batch 590/1684 - Avg Loss: 10.0139, Avg Acc: 0.5369\n",
      "[TRAIN] Batch 600/1684 - Avg Loss: 9.8586, Avg Acc: 0.5357\n",
      "[TRAIN] Batch 610/1684 - Avg Loss: 9.7084, Avg Acc: 0.5349\n",
      "[TRAIN] Batch 620/1684 - Avg Loss: 9.5629, Avg Acc: 0.5345\n",
      "[TRAIN] Batch 630/1684 - Avg Loss: 9.4219, Avg Acc: 0.5339\n",
      "[TRAIN] Batch 640/1684 - Avg Loss: 9.2856, Avg Acc: 0.5333\n",
      "[TRAIN] Batch 650/1684 - Avg Loss: 9.1534, Avg Acc: 0.5323\n",
      "[TRAIN] Batch 660/1684 - Avg Loss: 9.0252, Avg Acc: 0.5327\n",
      "[TRAIN] Batch 670/1684 - Avg Loss: 8.9008, Avg Acc: 0.5323\n",
      "[TRAIN] Batch 680/1684 - Avg Loss: 8.7801, Avg Acc: 0.5315\n",
      "[TRAIN] Batch 690/1684 - Avg Loss: 8.6629, Avg Acc: 0.5308\n",
      "[TRAIN] Batch 700/1684 - Avg Loss: 8.5491, Avg Acc: 0.5307\n",
      "[TRAIN] Batch 710/1684 - Avg Loss: 8.4384, Avg Acc: 0.5305\n",
      "[TRAIN] Batch 720/1684 - Avg Loss: 8.3308, Avg Acc: 0.5309\n",
      "[TRAIN] Batch 730/1684 - Avg Loss: 8.2262, Avg Acc: 0.5302\n",
      "[TRAIN] Batch 740/1684 - Avg Loss: 8.1244, Avg Acc: 0.5300\n",
      "[TRAIN] Batch 750/1684 - Avg Loss: 8.0253, Avg Acc: 0.5294\n",
      "[TRAIN] Batch 760/1684 - Avg Loss: 7.9288, Avg Acc: 0.5293\n",
      "[TRAIN] Batch 770/1684 - Avg Loss: 7.8349, Avg Acc: 0.5284\n",
      "[TRAIN] Batch 780/1684 - Avg Loss: 7.7433, Avg Acc: 0.5281\n",
      "[TRAIN] Batch 790/1684 - Avg Loss: 7.6540, Avg Acc: 0.5279\n",
      "[TRAIN] Batch 800/1684 - Avg Loss: 7.5670, Avg Acc: 0.5280\n",
      "[TRAIN] Batch 810/1684 - Avg Loss: 7.4828, Avg Acc: 0.5274\n",
      "[TRAIN] Batch 820/1684 - Avg Loss: 7.4000, Avg Acc: 0.5267\n",
      "[TRAIN] Batch 830/1684 - Avg Loss: 7.3192, Avg Acc: 0.5262\n",
      "[TRAIN] Batch 840/1684 - Avg Loss: 7.2403, Avg Acc: 0.5254\n",
      "[TRAIN] Batch 850/1684 - Avg Loss: 7.1633, Avg Acc: 0.5248\n",
      "[TRAIN] Batch 860/1684 - Avg Loss: 7.0881, Avg Acc: 0.5247\n",
      "[TRAIN] Batch 870/1684 - Avg Loss: 7.0146, Avg Acc: 0.5238\n",
      "[TRAIN] Batch 880/1684 - Avg Loss: 6.9427, Avg Acc: 0.5231\n",
      "[TRAIN] Batch 890/1684 - Avg Loss: 6.8760, Avg Acc: 0.5230\n",
      "[TRAIN] Batch 900/1684 - Avg Loss: 6.8073, Avg Acc: 0.5231\n",
      "[TRAIN] Batch 910/1684 - Avg Loss: 6.7401, Avg Acc: 0.5230\n",
      "[TRAIN] Batch 920/1684 - Avg Loss: 6.6744, Avg Acc: 0.5230\n",
      "[TRAIN] Batch 930/1684 - Avg Loss: 6.6100, Avg Acc: 0.5225\n",
      "[TRAIN] Batch 940/1684 - Avg Loss: 6.5471, Avg Acc: 0.5221\n",
      "[TRAIN] Batch 950/1684 - Avg Loss: 6.4855, Avg Acc: 0.5216\n",
      "[TRAIN] Batch 960/1684 - Avg Loss: 6.4251, Avg Acc: 0.5215\n",
      "[TRAIN] Batch 970/1684 - Avg Loss: 6.3660, Avg Acc: 0.5205\n",
      "[TRAIN] Batch 980/1684 - Avg Loss: 6.3081, Avg Acc: 0.5202\n",
      "[TRAIN] Batch 990/1684 - Avg Loss: 6.2514, Avg Acc: 0.5200\n",
      "[TRAIN] Batch 1000/1684 - Avg Loss: 6.1958, Avg Acc: 0.5199\n",
      "[TRAIN] Batch 1010/1684 - Avg Loss: 6.1414, Avg Acc: 0.5197\n",
      "[TRAIN] Batch 1020/1684 - Avg Loss: 6.0880, Avg Acc: 0.5195\n",
      "[TRAIN] Batch 1030/1684 - Avg Loss: 6.0355, Avg Acc: 0.5197\n",
      "[TRAIN] Batch 1040/1684 - Avg Loss: 5.9962, Avg Acc: 0.5194\n",
      "[TRAIN] Batch 1050/1684 - Avg Loss: 5.9554, Avg Acc: 0.5190\n",
      "[TRAIN] Batch 1060/1684 - Avg Loss: 5.9057, Avg Acc: 0.5190\n",
      "[TRAIN] Batch 1070/1684 - Avg Loss: 5.8570, Avg Acc: 0.5188\n",
      "[TRAIN] Batch 1080/1684 - Avg Loss: 5.8092, Avg Acc: 0.5190\n",
      "[TRAIN] Batch 1090/1684 - Avg Loss: 5.7622, Avg Acc: 0.5195\n",
      "[TRAIN] Batch 1100/1684 - Avg Loss: 5.7162, Avg Acc: 0.5190\n",
      "[TRAIN] Batch 1110/1684 - Avg Loss: 5.6709, Avg Acc: 0.5190\n",
      "[TRAIN] Batch 1120/1684 - Avg Loss: 5.6264, Avg Acc: 0.5185\n",
      "[TRAIN] Batch 1130/1684 - Avg Loss: 5.5828, Avg Acc: 0.5185\n",
      "[TRAIN] Batch 1140/1684 - Avg Loss: 5.5399, Avg Acc: 0.5180\n",
      "[TRAIN] Batch 1150/1684 - Avg Loss: 5.4977, Avg Acc: 0.5179\n",
      "[TRAIN] Batch 1160/1684 - Avg Loss: 5.4563, Avg Acc: 0.5176\n",
      "[TRAIN] Batch 1170/1684 - Avg Loss: 5.4156, Avg Acc: 0.5173\n",
      "[TRAIN] Batch 1180/1684 - Avg Loss: 5.3756, Avg Acc: 0.5169\n",
      "[TRAIN] Batch 1190/1684 - Avg Loss: 5.3363, Avg Acc: 0.5168\n",
      "[TRAIN] Batch 1200/1684 - Avg Loss: 5.2976, Avg Acc: 0.5166\n",
      "[TRAIN] Batch 1210/1684 - Avg Loss: 5.2595, Avg Acc: 0.5163\n",
      "[TRAIN] Batch 1220/1684 - Avg Loss: 5.2221, Avg Acc: 0.5161\n",
      "[TRAIN] Batch 1230/1684 - Avg Loss: 5.1853, Avg Acc: 0.5159\n",
      "[TRAIN] Batch 1240/1684 - Avg Loss: 5.1490, Avg Acc: 0.5158\n",
      "[TRAIN] Batch 1250/1684 - Avg Loss: 5.1133, Avg Acc: 0.5155\n",
      "[TRAIN] Batch 1260/1684 - Avg Loss: 5.0784, Avg Acc: 0.5156\n",
      "[TRAIN] Batch 1270/1684 - Avg Loss: 5.0438, Avg Acc: 0.5155\n",
      "[TRAIN] Batch 1280/1684 - Avg Loss: 5.0098, Avg Acc: 0.5154\n",
      "[TRAIN] Batch 1290/1684 - Avg Loss: 4.9764, Avg Acc: 0.5152\n",
      "[TRAIN] Batch 1300/1684 - Avg Loss: 4.9435, Avg Acc: 0.5151\n",
      "[TRAIN] Batch 1310/1684 - Avg Loss: 4.9111, Avg Acc: 0.5149\n",
      "[TRAIN] Batch 1320/1684 - Avg Loss: 4.8791, Avg Acc: 0.5148\n",
      "[TRAIN] Batch 1330/1684 - Avg Loss: 4.8476, Avg Acc: 0.5150\n",
      "[TRAIN] Batch 1340/1684 - Avg Loss: 4.8166, Avg Acc: 0.5145\n",
      "[TRAIN] Batch 1350/1684 - Avg Loss: 4.7869, Avg Acc: 0.5145\n",
      "[TRAIN] Batch 1360/1684 - Avg Loss: 4.7568, Avg Acc: 0.5144\n",
      "[TRAIN] Batch 1370/1684 - Avg Loss: 4.7284, Avg Acc: 0.5146\n",
      "[TRAIN] Batch 1380/1684 - Avg Loss: 4.6992, Avg Acc: 0.5140\n",
      "[TRAIN] Batch 1390/1684 - Avg Loss: 4.6703, Avg Acc: 0.5139\n",
      "[TRAIN] Batch 1400/1684 - Avg Loss: 4.6421, Avg Acc: 0.5134\n",
      "[TRAIN] Batch 1410/1684 - Avg Loss: 4.6141, Avg Acc: 0.5136\n",
      "[TRAIN] Batch 1420/1684 - Avg Loss: 4.5868, Avg Acc: 0.5136\n",
      "[TRAIN] Batch 1430/1684 - Avg Loss: 4.5595, Avg Acc: 0.5136\n",
      "[TRAIN] Batch 1440/1684 - Avg Loss: 4.5327, Avg Acc: 0.5133\n",
      "[TRAIN] Batch 1450/1684 - Avg Loss: 4.5062, Avg Acc: 0.5134\n",
      "[TRAIN] Batch 1460/1684 - Avg Loss: 4.4801, Avg Acc: 0.5135\n",
      "[TRAIN] Batch 1470/1684 - Avg Loss: 4.4543, Avg Acc: 0.5132\n",
      "[TRAIN] Batch 1480/1684 - Avg Loss: 4.4289, Avg Acc: 0.5131\n",
      "[TRAIN] Batch 1490/1684 - Avg Loss: 4.4038, Avg Acc: 0.5131\n",
      "[TRAIN] Batch 1500/1684 - Avg Loss: 4.3792, Avg Acc: 0.5129\n",
      "[TRAIN] Batch 1510/1684 - Avg Loss: 4.3548, Avg Acc: 0.5125\n",
      "[TRAIN] Batch 1520/1684 - Avg Loss: 4.3307, Avg Acc: 0.5122\n",
      "[TRAIN] Batch 1530/1684 - Avg Loss: 4.3069, Avg Acc: 0.5119\n",
      "[TRAIN] Batch 1540/1684 - Avg Loss: 4.2835, Avg Acc: 0.5116\n",
      "[TRAIN] Batch 1550/1684 - Avg Loss: 4.2603, Avg Acc: 0.5117\n",
      "[TRAIN] Batch 1560/1684 - Avg Loss: 4.2374, Avg Acc: 0.5114\n",
      "[TRAIN] Batch 1570/1684 - Avg Loss: 4.2149, Avg Acc: 0.5114\n",
      "[TRAIN] Batch 1580/1684 - Avg Loss: 4.1926, Avg Acc: 0.5115\n",
      "[TRAIN] Batch 1590/1684 - Avg Loss: 4.1706, Avg Acc: 0.5116\n",
      "[TRAIN] Batch 1600/1684 - Avg Loss: 4.1488, Avg Acc: 0.5114\n",
      "[TRAIN] Batch 1610/1684 - Avg Loss: 4.1274, Avg Acc: 0.5115\n",
      "[TRAIN] Batch 1620/1684 - Avg Loss: 4.1062, Avg Acc: 0.5115\n",
      "[TRAIN] Batch 1630/1684 - Avg Loss: 4.0852, Avg Acc: 0.5114\n",
      "[TRAIN] Batch 1640/1684 - Avg Loss: 4.0645, Avg Acc: 0.5109\n",
      "[TRAIN] Batch 1650/1684 - Avg Loss: 4.0441, Avg Acc: 0.5108\n",
      "[TRAIN] Batch 1660/1684 - Avg Loss: 4.0239, Avg Acc: 0.5108\n",
      "[TRAIN] Batch 1670/1684 - Avg Loss: 4.0040, Avg Acc: 0.5110\n",
      "[TRAIN] Batch 1680/1684 - Avg Loss: 3.9843, Avg Acc: 0.5108\n",
      "[TRAIN] Batch 1684/1684 - Avg Loss: 3.9777, Avg Acc: 0.5107\n",
      "[TRAIN] Epoch 1 summary: Loss: 3.9777, Acc: 0.5107\n",
      "[VALID] Batch 10/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 20/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 30/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 40/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 50/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 60/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 70/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 80/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 90/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 100/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 110/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 120/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 130/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 140/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 150/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 160/338 - Avg Loss: 0.6982, Avg Acc: 0.0000\n",
      "[VALID] Batch 170/338 - Avg Loss: 0.6981, Avg Acc: 0.0077\n",
      "[VALID] Batch 180/338 - Avg Loss: 0.6975, Avg Acc: 0.0628\n",
      "[VALID] Batch 190/338 - Avg Loss: 0.6970, Avg Acc: 0.1122\n",
      "[VALID] Batch 200/338 - Avg Loss: 0.6966, Avg Acc: 0.1566\n",
      "[VALID] Batch 210/338 - Avg Loss: 0.6962, Avg Acc: 0.1967\n",
      "[VALID] Batch 220/338 - Avg Loss: 0.6958, Avg Acc: 0.2332\n",
      "[VALID] Batch 230/338 - Avg Loss: 0.6955, Avg Acc: 0.2666\n",
      "[VALID] Batch 240/338 - Avg Loss: 0.6952, Avg Acc: 0.2971\n",
      "[VALID] Batch 250/338 - Avg Loss: 0.6949, Avg Acc: 0.3252\n",
      "[VALID] Batch 260/338 - Avg Loss: 0.6946, Avg Acc: 0.3512\n",
      "[VALID] Batch 270/338 - Avg Loss: 0.6944, Avg Acc: 0.3752\n",
      "[VALID] Batch 280/338 - Avg Loss: 0.6942, Avg Acc: 0.3975\n",
      "[VALID] Batch 290/338 - Avg Loss: 0.6940, Avg Acc: 0.4183\n",
      "[VALID] Batch 300/338 - Avg Loss: 0.6938, Avg Acc: 0.4377\n",
      "[VALID] Batch 310/338 - Avg Loss: 0.6936, Avg Acc: 0.4558\n",
      "[VALID] Batch 320/338 - Avg Loss: 0.6934, Avg Acc: 0.4729\n",
      "[VALID] Batch 330/338 - Avg Loss: 0.6933, Avg Acc: 0.4888\n",
      "[VALID] Batch 338/338 - Avg Loss: 0.6932, Avg Acc: 0.5001\n",
      "[VALID] Epoch 1 summary: Loss: 0.6932, Acc: 0.5001\n",
      "\n",
      "=== Epoch 2/20 ===\n",
      "[TRAIN] Batch 10/1684 - Avg Loss: 0.6937, Avg Acc: 0.4469\n",
      "[TRAIN] Batch 20/1684 - Avg Loss: 0.6933, Avg Acc: 0.4875\n",
      "[TRAIN] Batch 30/1684 - Avg Loss: 0.6931, Avg Acc: 0.5052\n",
      "[TRAIN] Batch 40/1684 - Avg Loss: 0.6931, Avg Acc: 0.5070\n",
      "[TRAIN] Batch 50/1684 - Avg Loss: 0.6965, Avg Acc: 0.5031\n",
      "[TRAIN] Batch 60/1684 - Avg Loss: 0.6961, Avg Acc: 0.4932\n",
      "[TRAIN] Batch 70/1684 - Avg Loss: 0.6956, Avg Acc: 0.4964\n",
      "[TRAIN] Batch 80/1684 - Avg Loss: 0.6953, Avg Acc: 0.5000\n",
      "[TRAIN] Batch 90/1684 - Avg Loss: 0.6951, Avg Acc: 0.5007\n",
      "[TRAIN] Batch 100/1684 - Avg Loss: 0.6949, Avg Acc: 0.4972\n",
      "[TRAIN] Batch 110/1684 - Avg Loss: 0.7070, Avg Acc: 0.5000\n",
      "[TRAIN] Batch 120/1684 - Avg Loss: 0.7058, Avg Acc: 0.5010\n",
      "[TRAIN] Batch 130/1684 - Avg Loss: 0.7048, Avg Acc: 0.5007\n",
      "[TRAIN] Batch 140/1684 - Avg Loss: 0.7040, Avg Acc: 0.5018\n",
      "[TRAIN] Batch 150/1684 - Avg Loss: 0.7033, Avg Acc: 0.5010\n",
      "[TRAIN] Batch 160/1684 - Avg Loss: 0.7027, Avg Acc: 0.5002\n",
      "[TRAIN] Batch 170/1684 - Avg Loss: 0.7021, Avg Acc: 0.4994\n",
      "[TRAIN] Batch 180/1684 - Avg Loss: 0.7016, Avg Acc: 0.4986\n",
      "[TRAIN] Batch 190/1684 - Avg Loss: 0.7012, Avg Acc: 0.5000\n",
      "[TRAIN] Batch 200/1684 - Avg Loss: 0.7008, Avg Acc: 0.5006\n",
      "[TRAIN] Batch 210/1684 - Avg Loss: 0.7004, Avg Acc: 0.5013\n",
      "[TRAIN] Batch 220/1684 - Avg Loss: 0.7001, Avg Acc: 0.5017\n",
      "[TRAIN] Batch 230/1684 - Avg Loss: 0.6998, Avg Acc: 0.5023\n",
      "[TRAIN] Batch 240/1684 - Avg Loss: 0.6995, Avg Acc: 0.5025\n",
      "[TRAIN] Batch 250/1684 - Avg Loss: 0.6992, Avg Acc: 0.5011\n",
      "[TRAIN] Batch 260/1684 - Avg Loss: 0.6990, Avg Acc: 0.5004\n",
      "[TRAIN] Batch 270/1684 - Avg Loss: 0.6988, Avg Acc: 0.5015\n",
      "[TRAIN] Batch 280/1684 - Avg Loss: 0.6986, Avg Acc: 0.4991\n",
      "[TRAIN] Batch 290/1684 - Avg Loss: 0.6984, Avg Acc: 0.4982\n",
      "[TRAIN] Batch 300/1684 - Avg Loss: 0.6982, Avg Acc: 0.4973\n",
      "[TRAIN] Batch 310/1684 - Avg Loss: 0.6981, Avg Acc: 0.4971\n",
      "[TRAIN] Batch 320/1684 - Avg Loss: 0.6979, Avg Acc: 0.4954\n",
      "[TRAIN] Batch 330/1684 - Avg Loss: 0.6978, Avg Acc: 0.4968\n",
      "[TRAIN] Batch 340/1684 - Avg Loss: 0.6977, Avg Acc: 0.4978\n",
      "[TRAIN] Batch 350/1684 - Avg Loss: 0.6975, Avg Acc: 0.4986\n",
      "[TRAIN] Batch 360/1684 - Avg Loss: 0.6974, Avg Acc: 0.4986\n",
      "[TRAIN] Batch 370/1684 - Avg Loss: 0.6973, Avg Acc: 0.4973\n",
      "[TRAIN] Batch 380/1684 - Avg Loss: 0.6972, Avg Acc: 0.4961\n",
      "[TRAIN] Batch 390/1684 - Avg Loss: 0.6971, Avg Acc: 0.4961\n",
      "[TRAIN] Batch 400/1684 - Avg Loss: 0.6970, Avg Acc: 0.4956\n",
      "[TRAIN] Batch 410/1684 - Avg Loss: 0.6969, Avg Acc: 0.4956\n",
      "[TRAIN] Batch 420/1684 - Avg Loss: 0.6968, Avg Acc: 0.4944\n",
      "[TRAIN] Batch 430/1684 - Avg Loss: 0.6968, Avg Acc: 0.4932\n",
      "[TRAIN] Batch 440/1684 - Avg Loss: 0.6967, Avg Acc: 0.4930\n",
      "[TRAIN] Batch 450/1684 - Avg Loss: 0.6966, Avg Acc: 0.4938\n",
      "[TRAIN] Batch 460/1684 - Avg Loss: 0.6965, Avg Acc: 0.4935\n",
      "[TRAIN] Batch 470/1684 - Avg Loss: 0.6964, Avg Acc: 0.4936\n",
      "[TRAIN] Batch 480/1684 - Avg Loss: 0.6964, Avg Acc: 0.4934\n",
      "[TRAIN] Batch 490/1684 - Avg Loss: 0.6963, Avg Acc: 0.4934\n",
      "[TRAIN] Batch 500/1684 - Avg Loss: 0.6963, Avg Acc: 0.4940\n",
      "[TRAIN] Batch 510/1684 - Avg Loss: 0.6963, Avg Acc: 0.4951\n",
      "[TRAIN] Batch 520/1684 - Avg Loss: 0.6962, Avg Acc: 0.4944\n",
      "[TRAIN] Batch 530/1684 - Avg Loss: 0.6961, Avg Acc: 0.4949\n",
      "[TRAIN] Batch 540/1684 - Avg Loss: 0.6961, Avg Acc: 0.4952\n",
      "[TRAIN] Batch 550/1684 - Avg Loss: 0.6962, Avg Acc: 0.4936\n",
      "[TRAIN] Batch 560/1684 - Avg Loss: 0.6961, Avg Acc: 0.4931\n",
      "[TRAIN] Batch 570/1684 - Avg Loss: 0.6961, Avg Acc: 0.4929\n",
      "[TRAIN] Batch 580/1684 - Avg Loss: 0.6960, Avg Acc: 0.4934\n",
      "[TRAIN] Batch 590/1684 - Avg Loss: 0.6960, Avg Acc: 0.4932\n",
      "[TRAIN] Batch 600/1684 - Avg Loss: 0.6959, Avg Acc: 0.4940\n",
      "[TRAIN] Batch 610/1684 - Avg Loss: 0.6983, Avg Acc: 0.4944\n",
      "[TRAIN] Batch 620/1684 - Avg Loss: 0.6983, Avg Acc: 0.4946\n",
      "[TRAIN] Batch 630/1684 - Avg Loss: 0.6982, Avg Acc: 0.4943\n",
      "[TRAIN] Batch 640/1684 - Avg Loss: 0.6981, Avg Acc: 0.4949\n",
      "[TRAIN] Batch 650/1684 - Avg Loss: 0.6980, Avg Acc: 0.4957\n",
      "[TRAIN] Batch 660/1684 - Avg Loss: 0.6979, Avg Acc: 0.4958\n",
      "[TRAIN] Batch 670/1684 - Avg Loss: 0.6979, Avg Acc: 0.4951\n",
      "[TRAIN] Batch 680/1684 - Avg Loss: 0.6978, Avg Acc: 0.4951\n",
      "[TRAIN] Batch 690/1684 - Avg Loss: 0.7022, Avg Acc: 0.4954\n",
      "[TRAIN] Batch 700/1684 - Avg Loss: 0.7021, Avg Acc: 0.4958\n",
      "[TRAIN] Batch 710/1684 - Avg Loss: 0.7020, Avg Acc: 0.4960\n",
      "[TRAIN] Batch 720/1684 - Avg Loss: 0.7019, Avg Acc: 0.4961\n",
      "[TRAIN] Batch 730/1684 - Avg Loss: 0.7017, Avg Acc: 0.4957\n",
      "[TRAIN] Batch 740/1684 - Avg Loss: 0.7016, Avg Acc: 0.4956\n",
      "[TRAIN] Batch 750/1684 - Avg Loss: 0.7015, Avg Acc: 0.4957\n",
      "[TRAIN] Batch 760/1684 - Avg Loss: 0.7014, Avg Acc: 0.4956\n",
      "[TRAIN] Batch 770/1684 - Avg Loss: 0.7013, Avg Acc: 0.4959\n",
      "[TRAIN] Batch 780/1684 - Avg Loss: 0.7012, Avg Acc: 0.4962\n",
      "[TRAIN] Batch 790/1684 - Avg Loss: 0.7011, Avg Acc: 0.4954\n",
      "[TRAIN] Batch 800/1684 - Avg Loss: 0.7010, Avg Acc: 0.4949\n",
      "[TRAIN] Batch 810/1684 - Avg Loss: 0.7009, Avg Acc: 0.4945\n",
      "[TRAIN] Batch 820/1684 - Avg Loss: 0.7008, Avg Acc: 0.4949\n",
      "[TRAIN] Batch 830/1684 - Avg Loss: 0.7007, Avg Acc: 0.4950\n",
      "[TRAIN] Batch 840/1684 - Avg Loss: 0.7006, Avg Acc: 0.4956\n",
      "[TRAIN] Batch 850/1684 - Avg Loss: 0.7005, Avg Acc: 0.4956\n",
      "[TRAIN] Batch 860/1684 - Avg Loss: 0.7004, Avg Acc: 0.4955\n",
      "[TRAIN] Batch 870/1684 - Avg Loss: 0.7003, Avg Acc: 0.4960\n",
      "[TRAIN] Batch 880/1684 - Avg Loss: 0.7003, Avg Acc: 0.4957\n",
      "[TRAIN] Batch 890/1684 - Avg Loss: 0.7002, Avg Acc: 0.4954\n",
      "[TRAIN] Batch 900/1684 - Avg Loss: 0.7001, Avg Acc: 0.4957\n",
      "[TRAIN] Batch 910/1684 - Avg Loss: 0.7000, Avg Acc: 0.4963\n",
      "[TRAIN] Batch 920/1684 - Avg Loss: 0.6999, Avg Acc: 0.4968\n",
      "[TRAIN] Batch 930/1684 - Avg Loss: 0.6999, Avg Acc: 0.4968\n",
      "[TRAIN] Batch 940/1684 - Avg Loss: 0.6998, Avg Acc: 0.4964\n",
      "[TRAIN] Batch 950/1684 - Avg Loss: 0.6997, Avg Acc: 0.4963\n",
      "[TRAIN] Batch 960/1684 - Avg Loss: 0.6997, Avg Acc: 0.4966\n",
      "[TRAIN] Batch 970/1684 - Avg Loss: 0.6996, Avg Acc: 0.4969\n",
      "[TRAIN] Batch 980/1684 - Avg Loss: 0.6995, Avg Acc: 0.4966\n",
      "[TRAIN] Batch 990/1684 - Avg Loss: 0.6995, Avg Acc: 0.4966\n",
      "[TRAIN] Batch 1000/1684 - Avg Loss: 0.6994, Avg Acc: 0.4963\n",
      "[TRAIN] Batch 1010/1684 - Avg Loss: 0.6993, Avg Acc: 0.4964\n",
      "[TRAIN] Batch 1020/1684 - Avg Loss: 0.6993, Avg Acc: 0.4968\n",
      "[TRAIN] Batch 1030/1684 - Avg Loss: 0.6992, Avg Acc: 0.4966\n",
      "[TRAIN] Batch 1040/1684 - Avg Loss: 0.6992, Avg Acc: 0.4969\n",
      "[TRAIN] Batch 1050/1684 - Avg Loss: 0.6991, Avg Acc: 0.4966\n",
      "[TRAIN] Batch 1060/1684 - Avg Loss: 0.6991, Avg Acc: 0.4962\n",
      "[TRAIN] Batch 1070/1684 - Avg Loss: 0.6990, Avg Acc: 0.4961\n",
      "[TRAIN] Batch 1080/1684 - Avg Loss: 0.6989, Avg Acc: 0.4964\n",
      "[TRAIN] Batch 1090/1684 - Avg Loss: 0.6989, Avg Acc: 0.4962\n",
      "[TRAIN] Batch 1100/1684 - Avg Loss: 0.6988, Avg Acc: 0.4963\n",
      "[TRAIN] Batch 1110/1684 - Avg Loss: 0.6988, Avg Acc: 0.4959\n",
      "[TRAIN] Batch 1120/1684 - Avg Loss: 0.6987, Avg Acc: 0.4957\n",
      "[TRAIN] Batch 1130/1684 - Avg Loss: 0.6987, Avg Acc: 0.4958\n",
      "[TRAIN] Batch 1140/1684 - Avg Loss: 0.6986, Avg Acc: 0.4959\n",
      "[TRAIN] Batch 1150/1684 - Avg Loss: 0.6986, Avg Acc: 0.4960\n",
      "[TRAIN] Batch 1160/1684 - Avg Loss: 0.6986, Avg Acc: 0.4958\n",
      "[TRAIN] Batch 1170/1684 - Avg Loss: 0.6985, Avg Acc: 0.4960\n",
      "[TRAIN] Batch 1180/1684 - Avg Loss: 0.6985, Avg Acc: 0.4960\n",
      "[TRAIN] Batch 1190/1684 - Avg Loss: 0.6984, Avg Acc: 0.4960\n",
      "[TRAIN] Batch 1200/1684 - Avg Loss: 0.6984, Avg Acc: 0.4960\n",
      "[TRAIN] Batch 1210/1684 - Avg Loss: 0.6983, Avg Acc: 0.4961\n",
      "[TRAIN] Batch 1220/1684 - Avg Loss: 0.6983, Avg Acc: 0.4962\n",
      "[TRAIN] Batch 1230/1684 - Avg Loss: 0.6982, Avg Acc: 0.4965\n",
      "[TRAIN] Batch 1240/1684 - Avg Loss: 0.6982, Avg Acc: 0.4966\n",
      "[TRAIN] Batch 1250/1684 - Avg Loss: 0.6982, Avg Acc: 0.4964\n",
      "[TRAIN] Batch 1260/1684 - Avg Loss: 0.6981, Avg Acc: 0.4963\n",
      "[TRAIN] Batch 1270/1684 - Avg Loss: 0.6981, Avg Acc: 0.4960\n",
      "[TRAIN] Batch 1280/1684 - Avg Loss: 0.6980, Avg Acc: 0.4964\n",
      "[TRAIN] Batch 1290/1684 - Avg Loss: 0.6980, Avg Acc: 0.4961\n",
      "[TRAIN] Batch 1300/1684 - Avg Loss: 0.6980, Avg Acc: 0.4966\n",
      "[TRAIN] Batch 1310/1684 - Avg Loss: 0.6979, Avg Acc: 0.4965\n",
      "[TRAIN] Batch 1320/1684 - Avg Loss: 0.6979, Avg Acc: 0.4962\n",
      "[TRAIN] Batch 1330/1684 - Avg Loss: 0.6979, Avg Acc: 0.4961\n",
      "[TRAIN] Batch 1340/1684 - Avg Loss: 0.6978, Avg Acc: 0.4962\n",
      "[TRAIN] Batch 1350/1684 - Avg Loss: 0.6978, Avg Acc: 0.4960\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 17\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m xb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     18\u001b[0m correct      \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((preds \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m==\u001b[39m yb)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m total        \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m yb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop with verbose printouts\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    # ---- Training ----\n",
    "    for batch_idx, (xb, yb) in enumerate(train_loader, start=1):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss  = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        correct      += ((preds >= 0.5) == yb).sum().item()\n",
    "        total        += yb.size(0)\n",
    "\n",
    "        # Print batch progress every 64 batches\n",
    "        if batch_idx % 10 == 0 or batch_idx == len(train_loader):\n",
    "            avg_loss = running_loss / total\n",
    "            avg_acc  = correct / total\n",
    "            print(f\"[TRAIN] Batch {batch_idx}/{len(train_loader)} - \"\n",
    "                  f\"Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f}\")\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc  = correct / total\n",
    "    print(f\"[TRAIN] Epoch {epoch} summary: Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (xb, yb) in enumerate(val_loader, start=1):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds   = model(xb)\n",
    "            loss    = criterion(preds, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            correct  += ((preds >= 0.5) == yb).sum().item()\n",
    "            total    += yb.size(0)\n",
    "\n",
    "            # Print validation progress every 10 batches\n",
    "            if batch_idx % 10 == 0 or batch_idx == len(val_loader):\n",
    "                avg_vloss = val_loss / total\n",
    "                avg_vacc  = correct / total\n",
    "                print(f\"[VALID] Batch {batch_idx}/{len(val_loader)} - \"\n",
    "                      f\"Avg Loss: {avg_vloss:.4f}, Avg Acc: {avg_vacc:.4f}\")\n",
    "\n",
    "    val_loss /= total\n",
    "    val_acc   = correct / total\n",
    "    print(f\"[VALID] Epoch {epoch} summary: Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "# Test\n",
    "model.eval()\n",
    "test_loss, correct, total = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds    = model(xb)\n",
    "        loss     = criterion(preds, yb)\n",
    "        test_loss += loss.item() * xb.size(0)\n",
    "        correct   += ((preds >= 0.5) == yb).sum().item()\n",
    "        total     += yb.size(0)\n",
    "test_loss /= total\n",
    "test_acc   = correct / total\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# Save\n",
    "torch.save(model.state_dict(), 'voice_magnitude_classifier.pth')\n",
    "print(\"Model saved to voice_magnitude_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5aacd-8c0a-4738-aaa8-3f3890cf1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== EVALUATING ON TEST SET ===\")\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (xb, yb) in enumerate(test_loader, 1):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        test_loss += loss.item() * xb.size(0)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        test_correct += (preds == yb).sum().item()\n",
    "        test_total += yb.size(0)\n",
    "        print(f\"[TEST] Batch {batch_idx}/{len(test_loader)} - Loss: {loss.item():.4f}\")\n",
    "test_loss /= test_total\n",
    "test_acc = test_correct / test_total\n",
    "print(f\"[TEST] → Final Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\\n\")\n",
    "\n",
    "print(\"=== SAVING MODEL ===\")\n",
    "torch.save(model.state_dict(), 'voice_magnitude_classifier.pth')\n",
    "print(\"[INFO] Model saved to voice_magnitude_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d445db36-6612-4f0d-90df-61a69ea6aca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluation Metrics ===\n",
      "Accuracy      : 0.7381\n",
      "Precision     : 0.6650\n",
      "Recall        : 0.9596\n",
      "F1-Score      : 0.7856\n",
      "ROC AUC Score : 0.9057\n",
      "\n",
      "Confusion Matrix:\n",
      "[[281 263]\n",
      " [ 22 522]] \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.93      0.52      0.66       544\n",
      "        real       0.66      0.96      0.79       544\n",
      "\n",
      "    accuracy                           0.74      1088\n",
      "   macro avg       0.80      0.74      0.72      1088\n",
      "weighted avg       0.80      0.74      0.72      1088\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# assume:\n",
    "#   model       – your trained PyTorch model\n",
    "#   test_loader – DataLoader for your test set\n",
    "#   device      – 'cuda' or 'cpu'\n",
    "\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_prob = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb).view(-1)       # shape (batch,)\n",
    "        probs = out.cpu().numpy()      # probabilities\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "        y_true.extend(yb.cpu().numpy())\n",
    "        y_pred.extend(preds)\n",
    "        y_prob.extend(probs)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_prob = np.array(y_prob)\n",
    "\n",
    "# Calculate metrics\n",
    "acc    = accuracy_score(y_true, y_pred)\n",
    "prec   = precision_score(y_true, y_pred)\n",
    "rec    = recall_score(y_true, y_pred)\n",
    "f1     = f1_score(y_true, y_pred)\n",
    "auc    = roc_auc_score(y_true, y_prob)\n",
    "cm     = confusion_matrix(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred, target_names=['fake','real'])\n",
    "\n",
    "# Print them\n",
    "print(\"=== Evaluation Metrics ===\")\n",
    "print(f\"Accuracy      : {acc:.4f}\")\n",
    "print(f\"Precision     : {prec:.4f}\")\n",
    "print(f\"Recall        : {rec:.4f}\")\n",
    "print(f\"F1-Score      : {f1:.4f}\")\n",
    "print(f\"ROC AUC Score : {auc:.4f}\\n\")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm, \"\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c4c29-a9ee-4636-b319-1c5123d39aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, glob\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 1) MODEL DEFINITION\n",
    "class VoiceClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=16000):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(64, 1)  # logits out\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "# 2) DATA LOADING\n",
    "def load_mag(dirpath):\n",
    "    X, y = [], []\n",
    "    for lbl, cls in enumerate(['fake','real']):\n",
    "        for f in glob.glob(os.path.join(dirpath,cls,'*.csv')):\n",
    "            df = pd.read_csv(f, usecols=['magnitude'])\n",
    "            X.append(df['magnitude'].values)\n",
    "            y.append(lbl)\n",
    "    return np.vstack(X).astype(np.float32), np.array(y)\n",
    "\n",
    "X_train, y_train = load_mag('./training')\n",
    "X_val,   y_val   = load_mag('./validation')\n",
    "X_test,  y_test  = load_mag('./testing')\n",
    "\n",
    "# 3) SCALE\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# 4) BALANCED SAMPLER\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights= 1. / class_counts\n",
    "sample_weights = class_weights[y_train]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "\n",
    "def make_loader(X,y, batch, sampler=None):\n",
    "    tx = torch.from_numpy(X)\n",
    "    ty = torch.from_numpy(y).float()\n",
    "    ds = TensorDataset(tx, ty)\n",
    "    return DataLoader(ds, batch, shuffle=(sampler is None), sampler=sampler)\n",
    "\n",
    "train_loader = make_loader(X_train, y_train, batch=64, sampler=sampler)\n",
    "val_loader   = make_loader(X_val,   y_val,   batch=64)\n",
    "test_loader  = make_loader(X_test,  y_test,  batch=64)\n",
    "\n",
    "# 5) SETUP\n",
    "device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model     = VoiceClassifier(X_train.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# 6) TRAIN/VAL LOOP WITH EARLY STOPPING\n",
    "best_val_acc, patience, counter = 0.0, 5, 0\n",
    "for epoch in range(1, 31):\n",
    "    # — TRAIN —\n",
    "    model.train()\n",
    "    all_preds, all_labels = [], []\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss   = criterion(logits, yb)\n",
    "        loss.backward(); optimizer.step()\n",
    "        preds = (torch.sigmoid(logits)>=0.5).long()\n",
    "        all_preds.append(preds.cpu().numpy()); all_labels.append(yb.cpu().numpy())\n",
    "    train_acc = accuracy_score(np.concatenate(all_labels), \n",
    "                               np.concatenate(all_preds))\n",
    "\n",
    "    # — VALIDATE —\n",
    "    model.eval()\n",
    "    v_preds, v_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            preds  = (torch.sigmoid(logits)>=0.5).long()\n",
    "            v_preds.append(preds.cpu().numpy()); v_labels.append(yb.cpu().numpy())\n",
    "    v_preds = np.concatenate(v_preds); v_labels = np.concatenate(v_labels)\n",
    "    val_acc = accuracy_score(v_labels, v_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc; counter = 0\n",
    "        torch.save(model.state_dict(),'best.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping!\") \n",
    "            break\n",
    "\n",
    "# 7) TEST WITH BEST MODEL\n",
    "model.load_state_dict(torch.load('best.pth'))\n",
    "model.eval()\n",
    "t_preds, t_labels, t_probs = [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        probs  = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds  = (probs>=0.5).astype(int)\n",
    "        t_probs.append(probs); t_preds.append(preds); t_labels.append(yb.cpu().numpy())\n",
    "\n",
    "t_preds  = np.concatenate(t_preds)\n",
    "t_labels = np.concatenate(t_labels)\n",
    "t_probs  = np.concatenate(t_probs)\n",
    "\n",
    "print(\"\\n=== Test Metrics ===\")\n",
    "print(\"Accuracy :\", accuracy_score(t_labels, t_preds))\n",
    "print(\"Precision:\", precision_score(t_labels, t_preds))\n",
    "print(\"Recall   :\", recall_score(t_labels, t_preds))\n",
    "print(\"F1-score :\", f1_score(t_labels, t_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
